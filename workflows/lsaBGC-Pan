#!/usr/bin/env python

### Program: lsaBGC-Pan
### Author: Rauf Salamzade
### Kalan Lab
### UW Madison, Department of Medical Microbiology and Immunology

# BSD 3-Clause License
#
# Copyright (c) 2024, Kalan-Lab
# All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are met:
#
# 1. Redistributions of source code must retain the above copyright notice, this
#    list of conditions and the following disclaimer.
#
# 2. Redistributions in binary form must reproduce the above copyright notice,
#    this list of conditions and the following disclaimer in the documentation
#    and/or other materials provided with the distribution.
#
# 3. Neither the name of the copyright holder nor the names of its
#    contributors may be used to endorse or promote products derived from
#    this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

import os
import sys
import argparse
import subprocess
import traceback
import multiprocessing
import math
import itertools
from Bio import SeqIO
from operator import itemgetter
from collections import defaultdict
from time import sleep
from lsaBGC import util
from ete3 import Tree
import warnings
warnings.filterwarnings('ignore')

def create_parser():
	""" Parse arguments """
	parser = argparse.ArgumentParser(description="""
       __              ___   _____  _____
      / /  ___ ___ _  / _ ) / ___/ / ___/
     / /  (_-</ _ `/ / _  |/ (_ / / /__  
    /_/  /___/\\_,_/ /____/ \\___/  \\___/  
    **********************************************************************************************************
    Program: lsaBGC-Pan
    Author: Rauf Salamzade
    Affiliation: Kalan Lab, UW Madison, Department of Medical Microbiology and Immunology
	
    QUICK DESCRIPTION:
    Workflow to run a pan-genome analysis of biosynthetic gene clusters for a single genus or species. 
    https://github.com/Kalan-Lab/lsaBGC-Pan/
		
    **********************************************************************************************************
    NOTES & CONSIDERATIONS:	

    * Check out the documentation at: https://github.com/Kalan-Lab/lsaBGC-Pan/wiki 	
    * Either provide an antiSMASH results directory or a directory with genomes (not both!).
    * lsaBGC-Pan does not have all the  functionalities of the original lsaBGC suite to make things simpler 
      and more straightforward. In particular, lsaBGC-(Auto)Expansion, which helped with scalability is 
      discluded because its usage generally requires more careful consideration and manual curation. Thus, to 
      make sure your analysis runs in a relatively timely manner, the program is restricted to handling data 
      from 4-200 genomes.
          - Consider using tools for dereplication/strain-clustering (such as skDER/CiDDER, dRep, PopPunk, 
            or treemmer) to prune down your set of genomes if you have more than 200.
    * Panaroo should only be used if working with genomes belonging to a single bacterial species. If working 
      with multiple bacterial species or fungal genomes, please use OrthoFinder instead.
    * Specifying fungal mode makes sure OrthoFinder is being used and that antiSMASH results were provided. 
      GECCO is not designed for fungal genomes. In addition, customized processing designed for bacterial 
      genomics is not performed and a more direct extraction of hierarchical orthogroups is applied.
    **********************************************************************************************************
    OVERVIEW OF WORKFLOW STEPS:	
	
    PART 1
    ----------------------------------------------------------------------------------------------------------
        - Step 1: Assess inputs provided
        - Step 1a: If genomes are provided, perform gene-calling with p(y)rodigal and annotate BGCs with GECCO
        - Step 1b: If antiSMASH results are provided, extracted genes from full genome GenBank files. If GECCO 
                   is requested, then it will also be run and overlapping GECCO and antiSMASH BGC regions will 
                   be consolidated by taking
                   the larger region. 
        - Step 2: Run OrthoFinder/Panaroo for orthology inference.
        - Step 3: Create species tree/phyogeny from (near-) core ortholog groups.
        - Step 4: Infer populations (will do so at multiple "core AAI" cutoffs) [no checkpoint, always rerun].
        - Step 5: Run lsaBGC-Cluster.py to determine evolutionary GCFs (by default in testing mode unless 
                  --auto-cluster specified) [no checkpoint, always rerun].
		
    BREAK (optional - but recommended - can be skipped by issuing --no-break)
    ----------------------------------------------------------------------------------------------------------
        - Step 6a: Manually examine which parameters make the most sense for evolutionary clustering of GCFs. 
                   Restart the workflow after with parameters for gene cluster clustering adapted.
        - Step 6b: Manually assess how population designations structure along the species tree with different 
                   core AAI cutoffs and, if desired, adjust population designations.							  

    PART 2
    ----------------------------------------------------------------------------------------------------------
        - Step 7: Parallel running of zol (twice with and without the -ace option) per GCF.
        - Step 8: Parallel running of GSeeF, lsaBGC-See, and lsaBGC-ComprehenSeeIve per GCF.
        - Step 9: Parallel running of lsaBGC-MIBiGMapper.
        - Step 10: Parallel running of lsaBGC-Reconcile and lsaBGC-Sociate.
        - Step 11: Create consolidated report of zol, lsaBGC-MIBiGMapper, lsaBGC-Reconcile, and lsaBGC-Sociate 
                   results [no checkpoint, always rerun].
    **********************************************************************************************************
	""", formatter_class=argparse.RawTextHelpFormatter)

	parser.add_argument('-a', '--antismash-results-directory', help="A directory with subdirectories corresponding to antiSMASH results\nper sample/genome [Optional].", required=False, default=None)
	parser.add_argument('-g', '--genomes-directory', help="A directory with additional genomes, e.g. those recently\nsequenced by the user, belonging to the taxa. Accepted formats\ninclude FASTA. Accepted suffices include: .fna, .fa, .fasta.\nWill run GECCO for\nBGC-predictions [Optional].", required=False, default=None)
	parser.add_argument('-o', '--output-directory', help="Parent output/workspace directory.", required=True)

	parser.add_argument('-f', '--fungal', action='store_true', help="Specify if input are from fungal genomes. Only possible\nif antiSMASH results are provided.", required=False, default=False)
	parser.add_argument('-k', '--keep-locus-tags', action='store_true', help="Keep original locus tags in antiSMASH GenBank files.", required=False, default=False)
	parser.add_argument('-rg', '--run-gecco', action='store_true', help="If antiSMASH results are provided also run GECCO for\nannotation of BGCs.", required=False, default=False)

	parser.add_argument('-up', '--use-panaroo', action='store_true', help='Use Panaroo instead of OrthoFinder for orthology inference.\nRecommended if investigating a single bacterial species.', default=False, required=False)
	parser.add_argument('-omc', '--run-coarse-orthofinder', action='store_true', help='Use coarse clustering for orthogroups in OrthoFinder instead\nof the more resolute hierarchical determined homolog groups.\nThere are some advantages to coarse OGs, including their\nconstruction being deterministic.', required=False, default=False)
	parser.add_argument('-ohq', '--run-msa-orthofinder', action='store_true', help='Run OrthoFinder using multiple sequence alignments instead of\nDendroBlast to determine hierarchical ortholog groups.', required=False, default=False)

	parser.add_argument('-hqp', '--high-quality-phylogeny', action='store_true', help='Prioritize quality over speed for phylogeny construction.', required=False, default=False)
	parser.add_argument('-cs', '--core-proportion', type=float, help='What proportion of genomes single-copy orthogroups need to be\nfound in to be used for species tree construction [Default is 0.9].', required=False, default=0.90)
	parser.add_argument('-mcs', '--max-core-genes', type=int, help="The maximum number of single copy (near-)core orthogroups to\nuse [Default is 500].", required=False, default=500)

	parser.add_argument('-pic', '--population-identity-cutoff', type=float, help='The core-genome identity cutoff used to define pairs of genomes as\nbelonging to the same group/population [Default is 99.0].', required=False, default=99.0)
	parser.add_argument('-mpf', '--manual-populations-file', help="Tab delimited file for manual mapping of samples to different\npopulations/clades.", required=False, default=None)

	parser.add_argument('-ci', '--cluster-inflation', type=float, help='The MCL inflation parameter for clustering BGCs into GCFs [Default\nis 0.8].', required=False, default=0.8)
	parser.add_argument('-cj', '--cluster-jaccard', type=float, help="Cutoff for Jaccard similarity of homolog groups shared between two\nBGCs [Default is 50.0].", required=False, default=50.0)
	parser.add_argument('-cr', '--cluster-syntenic-correlation', type=float, help='The minimal correlation coefficient needed between for considering them\nas a pair prior to MCL [Default is 0.4].', required=False, default=0.4)
	parser.add_argument('-cc', '--cluster-containment', type=float, help="Cutoff for percentage of OGs for a gene cluster near a contig edge\nto be found within the comparing gene cluster for the pair to be\nconsidered in MCL (a minimum of 3 OGs shared are still required) [Default\nis 70.0]", required=False, default=70.0)

	parser.add_argument('-nb', '--no-break', action='store_true', help='No break after step 5 to assess GCF clustering and population\nstratification and adapt parameters.', required=False, default=False)

	parser.add_argument('-zp', '--zol-parameters', help="The parameters to run zol analyses with - please surround by quotes\n[Defaut is \"\"].", default="")
	parser.add_argument('-zhq', '--zol-high-quality-preset', action='store_true', help='Use preset of options for performing high-quality and comprehensive zol\nanalyses instead of prioritizing speed.')
	parser.add_argument('-zl', '--zol-keep-multi-copy', action='store_true', help='Include all GCF instances in zol analysis, not just the most\nrepresentative instance from each sample/genome.', required=False, default=False)
	
	parser.add_argument('-ed', '--edge-distance', type=int, help="Distance in bp to scaffold/contig edge to be considered potentially\nfragmented. Used in GCF clustering (related to --cluster-containment\nparameter) and zol conservation computations [Default is 5000].", required=False, default=5000)
	# considering adding future argument for focal population definition in zol based on ID determined by popstrat

	parser.add_argument('-hqr', '--high-quality-reconcile', action='store_true', help="Perform high-quality alignment for reconcile analysis.", required=False, default=False)

	parser.add_argument('-py', '--use-prodigal', action='store_true', help='Use prodigal instead of pyrodigal (only if genomes are provided - not\nrelevant if antiSMASH results provided).', required=False, default=False)
	parser.add_argument('-c', '--threads', type=int, help="Total number of threads/processes to use. Recommend inreasing as much\nas possible. [Default is 4].", required=False, default=4)
	parser.add_argument('-d', '--docker-mode', action='store_true', help=argparse.SUPPRESS, required=False, default=False)
	
	args = parser.parse_args()
	return args

def lsaBGC():
	version_string = util.parseVersionFromSetupPy()
	if '-v' in sys.argv or '--version' in sys.argv:
		logo = """
   __              ___   _____  _____
  / /  ___ ___ _  / _ ) / ___/ / ___/
 / /  (_-</ _ `/ / _  |/ (_ / / /__  
/_/  /___/\\_,_/ /____/ \\___/  \\___/  
*************************************"""
		sys.stdout.write(logo + '\n\n')
		sys.stdout.write('Running lsaBGC-Pan version %s\n' % version_string)
		

	myargs = create_parser()

	### PARSE OPTIONS
	antismash_directory = myargs.antismash_results_directory
	genomes_directory = myargs.genomes_directory
	outdir = os.path.abspath(myargs.output_directory) + '/'

	fungal_flag = myargs.fungal
	run_gecco_flag = myargs.run_gecco
	use_prodigal = myargs.use_prodigal
	no_break_flag = myargs.no_break
	keep_lt_flag = myargs.keep_locus_tags

	use_panaroo_flag = myargs.use_panaroo
	run_coarse_orthofinder_flag = myargs.run_coarse_orthofinder
	run_msa_orthofinder_flag = myargs.run_msa_orthofinder
	
	cluster_inflation_param = myargs.cluster_inflation 
	cluster_jaccard_param = myargs.cluster_jaccard
	cluster_syntenic_correlation = myargs.cluster_syntenic_correlation
	cluster_containmment = myargs.cluster_containment

	high_quality_phylogeny_flag = myargs.high_quality_phylogeny
	phylogeny_core_proportion = myargs.core_proportion
	max_core_genes = myargs.max_core_genes
	population_identify_cutoff = myargs.population_identity_cutoff
	manual_populations_file = myargs.manual_populations_file

	zol_parameters = myargs.zol_parameters
	zol_high_quality_preset = myargs.zol_high_quality_preset
	zol_edge_distance = myargs.edge_distance
	zol_keep_multi_copy = myargs.zol_keep_multi_copy
	
	threads = myargs.threads
	docker_mode_flag = myargs.docker_mode

	if (antismash_directory != None and os.path.isdir(antismash_directory)) and (genomes_directory != None and os.path.isdir(genomes_directory)):
		msg = 'Both directories of antiSMASH results and genomes provided. This is not alllowed currently.'
		sys.stderr.write(msg + '\n')
		sys.exit(1)
	elif (antismash_directory != None and os.path.isdir(antismash_directory)):
		antismash_directory = os.path.abspath(antismash_directory) + '/'
		msg = 'antiSMASH results directory provided: %s' % antismash_directory
		sys.stderr.write(msg + '\n')
	elif (genomes_directory != None and os.path.isdir(genomes_directory)):
		genomes_directory = os.path.abspath(genomes_directory) + '/'
		msg = 'Genomes directory provided: %s' % genomes_directory
		sys.stderr.write(msg + '\n')

	if genomes_directory != None:
		try:
			genomes_directory = os.path.abspath(genomes_directory) + '/'
			assert (os.path.isdir(genomes_directory))
		except:
			sys.stderr.write('User defined genomes directory could not be validated to exist!\n')
			sys.exit(1)

	if os.path.isdir(outdir):
		sys.stderr.write(
			"Output directory already exists! Overwriting in 5 seconds, but only where needed, will use checkpoint files to skip certain steps...\n")
		sleep(5)
	else:
		util.setupReadyDirectory([outdir])

	# create logging object
	log_file = outdir + 'Progress.log'
	logObject = util.createLoggerObject(log_file)
	parameters_file = outdir + 'Command_Issued.txt'
	logo = """
   __              ___   _____  _____
  / /  ___ ___ _  / _ ) / ___/ / ___/
 / /  (_-</ _ `/ / _  |/ (_ / / /__  
/_/  /___/\\_,_/ /____/ \\___/  \\___/  
*************************************"""
	sys.stdout.write(logo + '\n\n')
	sys.stdout.write('Running lsaBGC-Pan version %s\n' % version_string)
	sys.stdout.write("Appending command issued for future records to: %s\n" % parameters_file)
	sys.stdout.write("Logging more details at: %s\n" % log_file)
	logObject.info("\nNEW RUN!!!\n**************************************")
	logObject.info('Running version %s' % version_string)
	logObject.info("Appending command issued for future records to: %s" % parameters_file)

	parameters_handle = open(parameters_file, 'a+')
	parameters_handle.write(' '.join(sys.argv) + '\n')
	parameters_handle.close()

	if fungal_flag:
		use_panaroo_flag = False
		try:
			assert(antismash_directory != None and os.path.isdir(antismash_directory))
		except:
			msg = 'fungal mode triggered but no antiSMASH results were provided. GECCO is not designed for fungi, so exiting ...'
			sys.stderr.write(msg + '\n')
			sys.exit(1)
	else:
		gw_pkl = outdir + 'GECCO_PF_Weights.pkl'
		axel_cmd = ['axel', 'https://github.com/Kalan-Lab/lsaBGC/raw/main/db/GECCO_PF_Weights.pkl', '-o', gw_pkl]
		try:
			util.runCmdViaSubprocess(axel_cmd, logObject=logObject, check_files=[gw_pkl])
		except:
			msg = 'Had issues downloading GECCO domain weights file!'
			sys.stderr.write(msg + '\n')
			sys.stderr.write(traceback.format_exc())
			sys.exit(1)
		os.environ["GECCO_DOMAIN_WEIGHTS_PKL_FILE"] = gw_pkl

	parallel_jobs_4thread = max(math.floor(threads / 4), 1)
	multi_thread = 4
	if threads < 4:
		multi_thread = threads
		parallel_jobs_4thread = 1

	if docker_mode_flag:
		msg = 'Running in Docker mode'
		sys.stdout.write(msg + '\n')
		logObject.info(msg)

	check_dir = outdir + 'Checkpoint_Files/'
	if not os.path.isdir(check_dir):
		os.mkdir(check_dir)

	scratch_dir = outdir + 'Scratch_Workspace/'
	if not os.path.isdir(scratch_dir):
		os.mkdir(scratch_dir)

	final_dir = outdir + 'Final_Results/'
	if not os.path.isdir(final_dir):
		os.mkdir(final_dir)

	# Step 1: process input genomes / antiSMASH results
	msg = '--------------------\nStep 1: Beginning by assessing input files\n--------------------'
	sys.stderr.write(msg + '\n')
	logObject.info(msg)

	genbank_recreate_dir = outdir + 'Recreated_GenBanks_with_LocusTags/'
	detailed_BGC_listing_file = outdir + 'Detailed_BGC_Listing.txt'
	simple_BGC_listing_file = outdir + 'Simple_BGC_Listing.txt'
	genome_wide_listing_file = outdir  + 'Sample_GenBank_Files.txt'
	step_1_checkpoint_file = check_dir + 'Step_1.txt'
	if not os.path.isfile(step_1_checkpoint_file):
		dblf_handle = open(detailed_BGC_listing_file, 'w')
		dblf_handle.write('\t'.join(['sample', 'method', 'genome_path', 'bgc_id', 'bgc_path', 'bgc_type', 'scaffold', 'start', 'end', 'bgc_length', 'dist_to_edge']) + '\n')
		sblf_handle = open(simple_BGC_listing_file, 'w')
		gwlf_handle = open(genome_wide_listing_file, 'w')
		if antismash_directory != None:
			msg_about_antismash_input = """ 
You have provided a directory with antiSMASH results - within which
we expect discrete subdirectories that each correspond to an individual 
sample. Furthermore, within each such subdirectory, multiple GenBank 
files are expected. Specifically, at least one GenBank file with 
'.region' substring in its name (corresponding to a BGC region) and 
one GenBank without the substring (corresponding to the full genome) 
are expected. If these expectations are not met, the subdirectory will 
be ignored. Note, the name of the subdirectory will be regarded as the 
sample name. GenBank files must also end with the suffix '.gbk'. In
addition, lsaBGC uses locus tags - which are automatically renamed 
to ensure seamless computing - if retention of original locus tags
is preferred, users can issue the --keep_locus_tags option.
			"""
			sys.stdout.write(msg_about_antismash_input + '\n')
			logObject.info(msg_about_antismash_input)

			sample_count = 0
			sample_bgc_regions = defaultdict(list)
			sample_genomes = {}
			for sample in os.listdir(antismash_directory):
				subdir = antismash_directory + sample + '/'
				if not os.path.isdir(subdir): continue
				genome_gbks = []
				region_gbks = []
				for f in os.listdir(subdir):
					if not f.endswith('.gbk'): continue
					gbk_file = subdir + f 
					if not os.path.isfile(gbk_file): continue
					if '.region' in f:
						region_gbks.append(gbk_file)
					else:
						genome_gbks.append(gbk_file)
				if len(genome_gbks) == 1 and len(region_gbks) >= 1:
					sample_count += 1
					sample_genomes[sample] = genome_gbks[0]
					sample_bgc_regions[sample] = region_gbks
				else:
					msg = 'Requirements for consideration not met for subdirectory: %s - ignoring it!' % sample
					sys.stderr.write(msg + '\n')
					logObject.warning(msg)

			if sample_count < 4 or sample_count > 200:
				msg = 'lsaBGC-Pan is designed for investigation of 4 to 200 genomes per analysis. Only found %d genomes.' % sample_count
				if sample_count > 200:
					msg = 'lsaBGC-Pan is designed for investigation of 4 to 200 genomes per analysis. Found %d genomes.' % sample_count
				sys.stderr.write(msg + '\n')
				logObject.error(msg)
				sys.exit(1)
			else:
				cds_lt_check_dir = scratch_dir + 'cds_locustag_checking/'
				scaff_inf_dir = scratch_dir + 'location_inference/'
				util.setupReadyDirectory([genbank_recreate_dir, cds_lt_check_dir, scaff_inf_dir])

				lt_check_inputs = []
				gbk_iter = 1
				for sample in sample_genomes:
					genome_gbk = sample_genomes[sample]
					outf = cds_lt_check_dir + str(gbk_iter) + '.txt'
					gbk_iter += 1
					lt_check_inputs.append([sample, 'genome', genome_gbk, outf, logObject])
					for bgc_gbk in sample_bgc_regions[sample]:
						outf = cds_lt_check_dir + str(gbk_iter) + '.txt'
						gbk_iter += 1
						lt_check_inputs.append([sample, 'bgc_region', bgc_gbk, outf, logObject])

				try:
					p = multiprocessing.Pool(threads)
					p.map(util.checkCDSHaveLocusTags, lt_check_inputs)
					p.close()
				except Exception as e:
					msg = 'Issues with assessing the validity and presence of locus tags in GenBank files.'
					sys.stderr.write(msg + '\n')
					logObject.error(msg)
					sys.stderr.write(traceback.format_exc())
					logObject.error(traceback.format_exc())
					sys.exit(1)

				samples_without_lts = set([])			
				for f in os.listdir(cds_lt_check_dir):
					with open(cds_lt_check_dir + f) as ocf:
						for line in ocf:
							line = line.strip()
							gbk_type, sample, gbk, cds_count, all_lt_found = line.split('\t')
							cds_count = int(cds_count)
							if cds_count == 0:
								if gbk_type == 'genome':
									del sample_genomes[sample]
									del sample_bgc_regions[sample]							
								else:
									sample_bgc_regions[sample].remove(gbk)
							elif not all_lt_found or not keep_lt_flag:
								samples_without_lts.add(sample)

				alphabet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'
				possible_locustags = sorted(list(set([''.join(list(x)) for x in list(itertools.product(alphabet, repeat=3))])))

				recreate_gbks_inputs = []		
				for i, sample in enumerate(samples_without_lts):
					if len(sample_bgc_regions) == 0:
						del sample_genomes[sample]
						del sample_bgc_regions[sample]
						continue
					lt = possible_locustags[i]
					recreate_gbks_inputs.append([sample, lt, genbank_recreate_dir, sample_genomes[sample]] + sample_bgc_regions[sample] + [keep_lt_flag, logObject])
					
				try:
					p = multiprocessing.Pool(threads)
					p.map(util.addLocusTagsToGBKs, recreate_gbks_inputs)
					p.close()
				except Exception as e:
					msg = 'Issues with recreating select GenBank files to make sure each CDS feature has locus tags.'
					sys.stderr.write(msg + '\n')
					logObject.error(msg)
					sys.stderr.write(traceback.format_exc())
					logObject.error(traceback.format_exc())
					sys.exit(1)

				if not keep_lt_flag:
					sample_genomes = {}
					sample_bgc_regions = defaultdict(list)

				sample_count = 0 
				for sample in os.listdir(genbank_recreate_dir):
					subdir = genbank_recreate_dir + sample + '/'
					if not os.path.isdir(subdir): continue
					genome_gbks = []
					region_gbks = []
					for f in os.listdir(subdir):
						if not f.endswith('.gbk'): continue
						gbk_file = subdir + f 
						if not os.path.isfile(gbk_file): continue
						if '.region' in f:
							region_gbks.append(gbk_file)
						else:
							genome_gbks.append(gbk_file)
					if len(genome_gbks) == 1 and len(region_gbks) >= 1:
						sample_count += 1
						sample_genomes[sample] = genome_gbks[0]
						sample_bgc_regions[sample] = region_gbks
					else:
						msg = 'Requirements for consideration not met for subdirectory following attempts to add locus tags to GenBanks: %s - ignoring it!' % sample
						sys.stderr.write(msg + '\n')
						logObject.warning(msg)

				if sample_count < 4 or sample_count > 200:
					msg = 'lsaBGC-Pan is designed for investigation of 4 to 200 genomes per analysis. Only found %d genomes.' % sample_count
					if sample_count > 200:
						msg = 'lsaBGC-Pan is designed for investigation of 4 to 200 genomes per analysis. Found %d genomes.' % sample_count
					sys.stderr.write(msg + '\n')
					logObject.error(msg)
					sys.exit(1)

				scaff_inf_inputs = []
				antismash_location_data = []
				for sample in sample_genomes:
					genome_gbk = sample_genomes[sample]
					for bgc_gbk in sample_bgc_regions[sample]:
						outf = scaff_inf_dir + sample + '_antismash_bgc_region_' + bgc_gbk.split('/')[-1]
						scaff_inf_inputs.append([genome_gbk, bgc_gbk, outf])
						antismash_location_data.append([sample, genome_gbk, bgc_gbk, outf])
				try:
					p = multiprocessing.Pool(threads)
					p.map(util.findAntiSMASHBGCInFullGenbank, scaff_inf_inputs)
					p.close()
				except Exception as e:
					msg = 'Issues with parallel determination of the location of antiSMASH BGCs.'
					sys.stderr.write(msg + '\n')
					logObject.error(msg)
					sys.stderr.write(traceback.format_exc())
					logObject.error(traceback.format_exc())
					sys.exit(1)

				antismash_bgc_locations = util.processAntiSMAHSBGCtoGenomeMappingResults(antismash_location_data, logObject)
				gecco_bgc_locations = []

				if run_gecco_flag:
					gecco_results_dir = outdir + 'GECCO_Results/'
					util.setupReadyDirectory([gecco_results_dir])
					gecco_cmds = []
					for sample in sample_genomes:
						genome_gbk = sample_genomes[sample]
						gecco_cmd = ['gecco', 'run', '-j', str(multi_thread), '-o', gecco_results_dir + sample + '/', 
				                     '-g', genome_gbk, '--cds-feature', 'CDS', logObject]
						gecco_cmds.append(gecco_cmd)

					try:
						p = multiprocessing.Pool(parallel_jobs_4thread)
						p.map(util.multiProcess, gecco_cmds)
						p.close()
					except:
						msg = 'Issues with parallel running of GECCO commands.'
						sys.stderr.write(msg + '\n')
						logObject.error(msg)
						sys.stderr.write(traceback.format_exc())
						logObject.error(traceback.format_exc())
						sys.exit(1)	

					for sample in os.listdir(gecco_results_dir):
						genome_gbk = sample_genomes[sample]
						scaff_lengths = {}
						with open(genome_gbk) as ogg:
							for rec in SeqIO.parse(ogg, 'genbank'):
								scaff_lengths[rec.id] = len(str(rec.seq))

						gecco_sample_results_dir = gecco_results_dir + sample + '/'
						for f in os.listdir(gecco_sample_results_dir):
							if not f.endswith('.clusters.tsv'): continue
							with open(gecco_sample_results_dir + f) as ogsrf:
								for i, line in enumerate(ogsrf):
									if i == 0: continue
									line = line.strip()
									scaff, bgc_id, start_coord, end_coord = line.split('\t')[:4]
									start_coord = int(start_coord)
									end_coord = int(end_coord)
									bgc_file = gecco_sample_results_dir + bgc_id + '.gbk'
									start_coord, end_coord = sorted([start_coord, end_coord])
									bgc_length = end_coord-start_coord+1
									dist_to_edge = min([int(start_coord), scaff_lengths[scaff]-int(end_coord)])
									gecco_bgc_locations.append([bgc_file, sample, scaff, start_coord, end_coord, bgc_length, dist_to_edge, 'gecco'])
						
				for sample in sample_genomes:
					gwlf_handle.write(sample + '\t' + sample_genomes[sample] + '\n')

				# conslidate results (picking larger regions if two overlap) and write to listing files
				all_bgc_locations = antismash_bgc_locations + gecco_bgc_locations

				accounted_positions = defaultdict(lambda: defaultdict(set))
				for bgc_info in sorted(all_bgc_locations, key=itemgetter(5), reverse=True):
					bgc_file, sample, scaff, start_coord, end_coord, bgc_length, dist_to_edge, prediction_software = bgc_info
					bgc_type = 'NA'
					if prediction_software.upper() == "ANTISMASH":
						bgc_type = util.parseAntiSMASHGBKForFunction(bgc_file, logObject, compress_multi=False)
					else:
						bgc_type = util.parseGECCOGBKForFunction(bgc_file, logObject)
					ap = 0
					for pos in range(start_coord, end_coord+1):
						if pos in accounted_positions[sample][scaff]:
							ap += 1
						accounted_positions[sample][scaff].add(pos)
					ap_prop = ap/float(bgc_length)
					if ap_prop >= 0.1:
						msg = 'Over 10%% of BGC %s (predicted via %s) overlaps in genome space with other longer BGCs and is thus dropped.' % (bgc_file, prediction_software)
						sys.stderr.write(msg + '\n')
						logObject.warning(msg)
						continue
					bgc_id = '.gbk'.join(bgc_file.split('/')[-1].split('.gbk')[:-1])
					dblf_handle.write('\t'.join([str(x) for x in [sample, prediction_software, sample_genomes[sample], bgc_id, bgc_file, bgc_type, scaff, start_coord, end_coord, bgc_length, dist_to_edge]]) + '\n')
					sblf_handle.write('\t'.join([sample, bgc_file, prediction_software, str(dist_to_edge)]) + '\n')
		else:
			gene_call_dir = outdir + 'Gene_Calling/'

			util.setupReadyDirectory(([gene_call_dir]))
			run_prodigal_cmds = []

			alphabet = 'ABCDEFGHIJKLMNOPQRSTUVWXYZ'
			possible_locustags = sorted(list(set([''.join(list(x)) for x in list(itertools.product(alphabet, repeat=3))])))

			sample_genomes = {}
			for i, file in enumerate(os.listdir(genomes_directory)):
				sample = util.cleanUpSampleName('.'.join(file.split('.')[:-1]))
				genome_file = genomes_directory + file
				try:
					assert (util.is_fasta(genome_file))
				except:
					msg = 'The file %s does not seem to be in FASTA format' % genome_file
					logObject.warning(msg + '\n')
					continue
				sample_genomes[sample] = gene_call_dir + sample + '.gbk'
				rpc = ['runProdigalAndMakeProperGenbank.py', '-i', genome_file, '-s', sample, 
		               '-o', gene_call_dir, '-l', possible_locustags[i], logObject]
				if use_prodigal:
					rpc += ['-gcm', 'prodigal']

				run_prodigal_cmds.append(rpc)
			
			sample_count = len(sample_genomes)
			if sample_count < 4 or sample_count > 200:
				msg = 'lsaBGC-Pan is designed for investigation of 4 to 200 genomes per analysis. Only found %d genomes.' % sample_count
				if sample_count > 200:
					msg = 'lsaBGC-Pan is designed for investigation of 4 to 200 genomes per analysis. Found %d genomes.' % sample_count
				sys.stderr.write(msg + '\n')
				logObject.error(msg)
				sys.exit(1)

			try:
				p = multiprocessing.Pool(threads)
				p.map(util.multiProcess, run_prodigal_cmds)
				p.close()
			except:
				msg = 'Issues with running gene calling.'
				sys.stderr.write(msg + '\n')
				logObject.error(msg)
				sys.stderr.write(traceback.format_exc())
				logObject.error(traceback.format_exc())
				sys.exit(1)	

			gecco_results_dir = outdir + 'GECCO_Results/'
			util.setupReadyDirectory([gecco_results_dir])
			gecco_cmds = []
			for sample in sample_genomes:
				genome_gbk = sample_genomes[sample]
				gecco_cmd = ['gecco', 'run', '-j', str(multi_thread), '-o', gecco_results_dir + sample + '/', 
		                    '-g', genome_gbk, '--cds-feature', 'CDS', logObject]
				gecco_cmds.append(gecco_cmd)

			try:
				p = multiprocessing.Pool(parallel_jobs_4thread)
				p.map(util.multiProcess, gecco_cmds)
				p.close()
			except:
				msg = 'Issues with parallel running of GECCO commands.'
				sys.stderr.write(msg + '\n')
				logObject.error(msg)
				sys.stderr.write(traceback.format_exc())
				logObject.error(traceback.format_exc())
				sys.exit(1)	

			gecco_bgc_locations = []
			for sample in os.listdir(gecco_results_dir):
				genome_gbk = sample_genomes[sample]
				scaff_lengths = {}
				with open(genome_gbk) as ogg:
					for rec in SeqIO.parse(ogg, 'genbank'):
						scaff_lengths[rec.id] = len(str(rec.seq))

				gecco_sample_results_dir = gecco_results_dir + sample + '/'
				for f in os.listdir(gecco_sample_results_dir):
					if not f.endswith('.clusters.tsv'): continue
					with open(gecco_sample_results_dir + f) as ogsrf:
						for i, line in enumerate(ogsrf):
							if i == 0: continue
							line = line.strip()
							scaff, bgc_id, start_coord, end_coord = line.split('\t')[:4]
							start_coord = int(start_coord)
							end_coord = int(end_coord)
							bgc_file = gecco_sample_results_dir + bgc_id + '.gbk'
							start_coord, end_coord = sorted([start_coord, end_coord])
							bgc_length = int(end_coord)-int(start_coord)+1
							dist_to_edge = min([int(start_coord), scaff_lengths[scaff]-int(end_coord)])
							gecco_bgc_locations.append([bgc_file, sample, scaff, start_coord, end_coord, bgc_length, dist_to_edge, 'gecco'])

			for sample in sample_genomes:
				gwlf_handle.write(sample + '\t' + sample_genomes[sample] + '\n')

			accounted_positions = defaultdict(lambda: defaultdict(set))
			for bgc_info in sorted(gecco_bgc_locations, key=itemgetter(5), reverse=True):
				bgc_file, sample, scaff, start_coord, end_coord, bgc_length, dist_to_edge, prediction_software = bgc_info
				bgc_type = 'NA'
				if prediction_software.upper() == "ANTISMASH":
					bgc_type = util.parseAntiSMASHGBKForFunction(bgc_gbk, logObject, compress_multi=False)
				else:
					bgc_type = util.parseGECCOGBKForFunction(bgc_file, logObject)
				ap = 0
				for pos in range(int(start_coord), int(end_coord)+1):
					if pos in accounted_positions[sample][scaff]:
						ap += 1
					accounted_positions[sample][scaff].add(pos)
				ap_prop = ap/float(bgc_length)
				if ap_prop >= 0.1:
					msg = 'Over 10%% of BGC %s (predicted via %s) overlaps in genome space with other longer BGCs and is thus dropped.' % (bgc_file, prediction_software)
					sys.stderr.write(msg + '\n')
					logObject.warning(msg)
					continue
				bgc_id = '.gbk'.join(bgc_file.split('/')[-1].split('.gbk')[:-1])
				dblf_handle.write('\t'.join([str(x) for x in [sample, prediction_software, sample_genomes[sample], bgc_id, bgc_file, bgc_type, scaff, start_coord, end_coord, bgc_length, dist_to_edge]]) + '\n')
				sblf_handle.write('\t'.join([sample, bgc_file, prediction_software, str(dist_to_edge)]) + '\n')

		gwlf_handle.close()
		sblf_handle.close()
		dblf_handle.close()
				
		os.system('touch %s' % step_1_checkpoint_file)

	# Step 2: Perform orthology inference
	msg = '--------------------\nStep 2: Performing orthology inference\n--------------------'
	sys.stderr.write(msg + '\n')
	logObject.info(msg)
	
	ortholog_matrix_file = outdir + 'Sample_by_Orthogroups_Matrix.tsv'
	step_2_checkpoint_file = check_dir + 'Step_2.txt'
	if not os.path.isfile(step_2_checkpoint_file):
		if not use_panaroo_flag:
			proteomes_dir = scratch_dir + 'orthofinder_inputs/'
			orthofinder_results_dir = outdir + 'OrthoFinder_Results/'
			util.setupReadyDirectory([proteomes_dir])
			
			sample_genomes = {}
			with open(detailed_BGC_listing_file) as odlf:
				for i, line in enumerate(odlf):
					if i == 0: continue
					line = line.strip()
					sample, _, genome_gbk = line.split('\t')[:3]
					sample_genomes[sample] = genome_gbk

			extract_cds_inputs = []
			for sample in sample_genomes:
				genome_gbk = sample_genomes[sample]
				proteome_faa = proteomes_dir + sample + '.faa'
				extract_cds_inputs.append([genome_gbk, proteome_faa, logObject])

			try:
				p = multiprocessing.Pool(threads)
				p.map(util.extractProteinsFromGenBank, extract_cds_inputs)
				p.close()
			except:
				msg = 'Issues with parallel extraction of CDS proteins into FASTA files from GenBank files.'
				sys.stderr.write(msg + '\n')
				logObject.error(msg)
				sys.stderr.write(traceback.format_exc())
				logObject.error(traceback.format_exc())
				sys.exit(1)	

			# check that there are not equivalent/overlapping CDS names within/across genomes
			cds_observation_count = defaultdict(int)
			for f in os.listdir(proteomes_dir):
				with open(proteomes_dir + f) as opf:
					for line in opf:
						line = line.strip()
						if line.startswith('>'):
							cds = line[1:].split()[0]
							if cds_observation_count[cds] > 0:
								msg = 'Error: CDS %s found multiple times in one or more genomes/proteomes.' % cds
								logObject.info(msg)
								sys.stderr.write(msg + '\n')
								sys.exit(1)
							cds_observation_count[cds] += 1

			og_results_matrix = None
			if run_coarse_orthofinder_flag:
				og_results_matrix = util.runOrthoFinder2(proteomes_dir, orthofinder_results_dir, logObject, threads=threads)
			else:	
				if fungal_flag:
					og_results_matrix = util.runOrthoFinder2FullFungal(proteomes_dir, orthofinder_results_dir, run_msa_orthofinder_flag, logObject, threads=threads)
				else:
					og_results_matrix = util.runOrthoFinder2Full(proteomes_dir, orthofinder_results_dir, run_msa_orthofinder_flag, logObject, threads=threads)
			os.system('mv %s %s' % (og_results_matrix, ortholog_matrix_file))
		else:
			panaroo_input_dir = scratch_dir + 'panaroo_inputs/'
			panaroo_results_dir = outdir + 'Panaroo_Results/'
			util.setupReadyDirectory([panaroo_input_dir])

			og_results_matrix = util.runPanaroo(detailed_BGC_listing_file, panaroo_input_dir, panaroo_results_dir, logObject, threads=threads, panaroo_options='--clean-mode moderate --remove-invalid-genes')
			os.system('mv %s %s' % (og_results_matrix, ortholog_matrix_file))
		
		try:
			assert(os.path.isfile(ortholog_matrix_file))
		except:
			msg = 'Issues validating that the final ortholog by sample matrix exists: %s' % ortholog_matrix_file
			sys.stderr.write(msg + '\n')
			logObject.info(msg)
			sys.exit(1)

		os.system('touch %s' % step_2_checkpoint_file)

	# Step 3: Create phylogeny
	msg = '--------------------\nStep 3: Creating phylogeny from (near-)core ortholog groups\n--------------------'
	sys.stderr.write(msg + '\n')
	logObject.info(msg)
	step_3_checkpoint_file = check_dir + 'Step_3.txt'
	phylogenate_results_dir = outdir + 'Create_Species_Phylogeny/'
	if not os.path.isfile(step_3_checkpoint_file):
		phylogenate_cmd = ['phylogenate', '-m', ortholog_matrix_file, '-i', genome_wide_listing_file, '-mg', str(max_core_genes),
					       '-p', str(phylogeny_core_proportion), '-c', str(threads), '-o', phylogenate_results_dir]

		species_tree_file = phylogenate_results_dir + 'Species_Phylogeny_FastTree2.treefile'
		if high_quality_phylogeny_flag:
			phylogenate_cmd += ['-hq']
			species_tree_file = phylogenate_results_dir + 'Species_Phylogeny_IQTREE_Phylogeny_mset-WAG-LG_B-1000.treefile'

		util.runCmdViaSubprocess(phylogenate_cmd, logObject=logObject, check_files=[species_tree_file])
		os.system('touch %s' % step_3_checkpoint_file)

	concat_core_alignment_fasta_file = phylogenate_results_dir + 'Concatenated_MSA.fasta'
	species_tree_file = phylogenate_results_dir + 'Species_Phylogeny_FastTree2.treefile'
	if not os.path.isfile(species_tree_file):
		species_tree_file = phylogenate_results_dir + 'Species_Phylogeny_IQTREE_Phylogeny_mset-WAG-LG_B-1000.treefile'
	
	try:
		assert(os.path.isfile(species_tree_file) or os.path.isfile(concat_core_alignment_fasta_file))
	except:
		msg = 'Issue validating the existence of species phylogeny or concatenated core FASTA files'
		logObject.error(msg)
		sys.stderr.write(msg + '\n')
		sys.exit(1)

	# Step 4: Determine populations
	msg = '--------------------\nStep 4: Stratifying populations\n--------------------'
	sys.stderr.write(msg + '\n')
	logObject.info(msg)

	popstrat_results_dir = outdir + 'Delineate_Populations/'
	popstrat_cmd = ['popstrat', '-a', concat_core_alignment_fasta_file, '-p', species_tree_file, 
					'-t', '-i', str(population_identify_cutoff), '-o', popstrat_results_dir]

	popstrat_file = popstrat_results_dir + 'Sample_to_Population.txt'
	util.runCmdViaSubprocess(popstrat_cmd, logObject=logObject, check_files=[popstrat_file])

	# Step 5: Cluster BGCs into GCFs
	msg = '--------------------\nStep 5: Clustering BGCs into GCFs\n--------------------'
	sys.stderr.write(msg + '\n')
	logObject.info(msg)

	cluster_results_dir = outdir + 'GCF_Clustering/'
	cluster_pair_file = cluster_results_dir + 'bgc_pair_relationships.txt'
	gcf_listing_dir = cluster_results_dir + 'GCF_Listings/'
	lsabgc_cluster_cmd = ['lsaBGC-Cluster', '-b', simple_BGC_listing_file, '-m', ortholog_matrix_file,
                          '-c', str(threads), '-o', cluster_results_dir, '-r', str(cluster_syntenic_correlation),
                          '-x', str(zol_edge_distance), '-i', str(cluster_inflation_param), 
						  '-y', str(cluster_containmment), '-j', str(cluster_jaccard_param), '-t']
 
	util.runCmdViaSubprocess(lsabgc_cluster_cmd, logObject=logObject, check_directories=[gcf_listing_dir], check_files=[cluster_pair_file])

	gcf_count = 0
	for f in os.listdir(gcf_listing_dir):
		gcf_count += 1
	
	try:
		assert(gcf_count > 0)
	except:
		msg = 'No GCFs found in the GCF_Listings/ directory at: %s' % gcf_listing_dir
		logObject.error(msg)
		sys.stderr.write(msg + '\n')
		sys.exit(1)

	bgc_id_to_gbk = {}
	sample_index = defaultdict(int)
	with open(simple_BGC_listing_file) as osblf:
		for i, line in enumerate(osblf):
			line = line.strip()
			try:
				assert (len(line.split('\t')) == 4)
			except Exception as e:
				msg = "More or less than three columns exist at line %d in BGC specification/listing file. Exiting now ..." % (i + 1)
				logObject.error(msg)
				logObject.error(traceback.format_exc())
				sys.stderr.write(msg + '\n')
				sys.exit(1)

			sample, bgc_gbk, _, _ = line.split('\t')
			sample = util.cleanUpSampleName(sample)
			assert (util.is_genbank(bgc_gbk))
			bgc_id = sample
			if sample_index[sample] > 0:
				bgc_id = sample + '_' + str(sample_index[sample] + 1)
			bgc_id_to_gbk[bgc_id] = bgc_gbk
			sample_index[sample] += 1

	pairwise_relations = defaultdict(lambda: defaultdict(float))
	with open(cluster_pair_file) as ocpf:
		for line in ocpf:
			line = line.strip()
			ls = line.split('\t')
			pairwise_relations[bgc_id_to_gbk[ls[0]]][bgc_id_to_gbk[ls[1]]] = float(ls[2])
	
	# Step 6: Taking a break for user examination of clustering and population delineation
	msg = '--------------------\nStep 6: Taking a break for user examination of GCF clustering and population delineations\n--------------------'
	sys.stderr.write(msg + '\n')
	logObject.info(msg)

	step_6_checkpoint_file = check_dir + 'Step_6.txt'
	if not os.path.isfile(step_6_checkpoint_file) and not no_break_flag:

		msg_about_manual_param_selection = """
Breaking workflow! 
------------------
This is to give you an opportunity to adjust parameters for GCF
clustering and population stratification based on manual assessment
of the PDF report showing the impact of parameters on clustering
found at: %s 
and PDF graphics of the species phylogeny with 
different population divisions marked which can be found in the 
directory %s. 
For more insight into how to select appropriate 
parameters, check out: 
https://github.com/Kalan-Lab/lsaBGC-Pan/wiki/7.-guide-to-parameter-selections-during-midway-break
		""" % (cluster_results_dir + '', popstrat_results_dir + '')
	
		logObject.info(msg_about_manual_param_selection)
		sys.stdout.write(msg_about_manual_param_selection + '\n')
		os.system('touch ' + step_6_checkpoint_file)
		sys.exit(0)

	bgc_to_gcf = {}
	for f in os.listdir(gcf_listing_dir):
		gcf_file = gcf_listing_dir + f
		gcf_id = f.split('.txt')[0]
		with open(gcf_file) as ogf:
			for line in ogf:
				bgc_path = line.strip()
				bgc_to_gcf[bgc_path] = gcf_id
	
	sample_to_pop = {}
	if manual_populations_file == None:
		with open(popstrat_file) as opf:
			for i, line in enumerate(opf):
				if i == 0: continue
				line = line.strip()
				ls = line.split('\t')
				sample_to_pop[ls[0]] = ls[1]
	else:
		sample_to_pop = defaultdict(lambda: 'Clade_Not_Provided')
		try:
			with open(manual_populations_file) as ompf:
				for line in ompf:
					line = line.strip()
					ls = line.split('\t')
					sample_to_pop[ls[0]] = ls[1]
		except:
			msg = 'Difficulties processing manual population specifications file.'
			sys.stderr.write(msg + '\n')
			sys.stderr.write(traceback.format_exc())
			logObject.error(msg)
			logObject.error(traceback.format_exc())

	detailed_BGC_listing_with_Pop_and_GCF_map_file = outdir + 'Detailed_BGC_Listing.with_Population_and_GCF_Labels.txt'
	dblf_handle = open(detailed_BGC_listing_with_Pop_and_GCF_map_file, 'w')
	
	dblf_handle.write('\t'.join(['sample', 'population', 'method', 'genome_path', 'bgc_id', 'bgc_path', 'bgc_type', 'gcf_id', 'scaffold', 'start', 'end', 'bgc_length', 'dist_to_edge']) + '\n')
	with open(detailed_BGC_listing_file) as odblf:
		for i, line in enumerate(odblf):
			if i == 0: continue
			line = line.strip()
			ls = line.split('\t')
			pop = sample_to_pop[ls[0]]
			gcf = bgc_to_gcf[ls[4]]
			updated_ls = [ls[0], pop] + ls[1:6] + [gcf] + ls[6:] 
			dblf_handle.write('\t'.join(updated_ls) + '\n')
	dblf_handle.close()
	
	# Step 7: Running zol
	msg = '--------------------\nStep 7: Running zol analyses\n--------------------'
	sys.stderr.write(msg + '\n')
	logObject.info(msg)
	step_7_checkpoint_file = check_dir + 'Step_7.txt'
	zol_results_dir = outdir + 'zol_Results/'
	zol_comp_results_dir = zol_results_dir + 'Comprehensive/'
	zol_full_results_dir = zol_results_dir + 'Complete_Instances/'
	if not os.path.isfile(step_7_checkpoint_file):
		util.setupReadyDirectory([zol_results_dir, zol_comp_results_dir, zol_full_results_dir])
		ortholog_listing_file = util.reformatOrthologInfo(ortholog_matrix_file, zol_results_dir, logObject)
		util.runZol(detailed_BGC_listing_with_Pop_and_GCF_map_file, ortholog_listing_file, pairwise_relations, zol_comp_results_dir, zol_full_results_dir, zol_parameters, zol_high_quality_preset, zol_edge_distance, zol_keep_multi_copy, multi_thread, parallel_jobs_4thread, logObject)
		os.system('touch %s' % step_7_checkpoint_file)

	# Step 8: Running GSeeF, lsaBGC-See and lsaBGC-ComprehenSeeIve
	msg = '--------------------\nStep 8: Running GSeeF, lsaBGC-See, and lsaBGC-ComprehenSeeIve analyses\n--------------------'
	sys.stderr.write(msg + '\n')
	logObject.info(msg)
	step_8_checkpoint_file = check_dir + 'Step_8.txt'
	visual_results_dir = final_dir + 'Visualizations/'
	gseef_dir = visual_results_dir + 'GSeeF_Results/'
	see_dir = visual_results_dir + 'lsaBGC_See_Results/'
	compsee_dir = visual_results_dir + 'lsaBGC_ComprehenSeeIve_Results/'
	if not os.path.isfile(step_8_checkpoint_file):
		util.setupReadyDirectory([visual_results_dir, see_dir, compsee_dir])
		gseef_cmd = ['GSeeF', '-g', detailed_BGC_listing_with_Pop_and_GCF_map_file, '-s', species_tree_file, '-o', gseef_dir]
		util.runCmdViaSubprocess(gseef_cmd, logObject, check_directories=[gseef_dir])
		util.runSeeAndComprehenSeeIve(detailed_BGC_listing_with_Pop_and_GCF_map_file, species_tree_file, ortholog_matrix_file, 
								      see_dir, compsee_dir, threads, logObject)
		os.system('touch %s' % step_8_checkpoint_file)

	# Step 9: Running lsaBGC-MIBiGMapper
	msg = '--------------------\nStep 9: Parallel running of lsaBGC-MIBiGMapper\n--------------------'
	sys.stderr.write(msg + '\n')
	logObject.info(msg)
	step_9_checkpoint_file = check_dir + 'Step_9.txt'
	mibig_dir = outdir + 'lsaBGC_MIBiG-Mapper/'
	if not os.path.isfile(step_9_checkpoint_file):
		util.setupReadyDirectory([mibig_dir])
		util.runMIBiGMapper(detailed_BGC_listing_with_Pop_and_GCF_map_file, ortholog_matrix_file, mibig_dir, 
					        multi_thread, parallel_jobs_4thread, logObject)
		os.system('touch %s' % step_9_checkpoint_file)

	# Step 10: Parallel running of lsaBGC-Reconcile (for horizontal transfer assessment) and lsaBGC-Sociate
	msg = '--------------------\nStep 10: Running lsaBGC-Reconcile and lsaBGC-Sociate\n--------------------'
	sys.stderr.write(msg + '\n')
	logObject.info(msg)
	step_10_checkpoint_file = check_dir + 'Step_10.txt'
	recon_dir = visual_results_dir + 'lsaBGC_Reconcile_Results/'
	recon_scratch_dir = scratch_dir + 'lsaBGC_Reconcile_Scratch/'
	recon_result_file = recon_dir + 'Orthogroup_Summary_Info.txt'
	recon_og_result_file = recon_dir + 'Orthogroup_by_Sample_Copy_Count.txt'
	recon_pop_color_file = recon_scratch_dir + 'pop_colors_mapping.txt'
	sociate_dir = outdir + 'lsaBGC_Sociate_Results/'
	sociate_result_file = sociate_dir + 'Final_Sociated_Results.tsv'
	if not os.path.isfile(step_10_checkpoint_file):
		util.setupReadyDirectory([recon_dir, recon_scratch_dir, sociate_dir])
		lsabgc_reconcile_cmd = ['lsaBGC-Reconcile', '-l', detailed_BGC_listing_with_Pop_and_GCF_map_file, '-m', 
						        ortholog_matrix_file, '-s', species_tree_file, '-o', recon_dir, '-sd', 
								recon_scratch_dir, '-c', str(threads)]
		util.runCmdViaSubprocess(lsabgc_reconcile_cmd, logObject=logObject, check_files=[recon_result_file, 
																				         recon_og_result_file,
																						 recon_pop_color_file])
		lsabgc_sociate_cmd = ['lsaBGC-Sociate', '-l', detailed_BGC_listing_with_Pop_and_GCF_map_file, '-m', 
						        ortholog_matrix_file, '-s', species_tree_file, '-o', sociate_dir, '-c', str(threads)]
		util.runCmdViaSubprocess(lsabgc_sociate_cmd, logObject=logObject, check_files=[sociate_result_file])
		os.system('touch %s' % step_10_checkpoint_file)
	
	# Step 11: Create consolidated report of zol, lsaBGC-MIBiGMapper, lsaBGC-Reconcile, and lsaBGC-Sociate results
	msg = '--------------------\nStep 11: Creating final spreadsheet with all the major results!\n--------------------'
	sys.stderr.write(msg + '\n')
	logObject.info(msg)

	final_spreadsheet_xlsx = final_dir + 'Consolidated_Spreadsheets.xlsx'
	util.createFinalSpreadsheets(detailed_BGC_listing_with_Pop_and_GCF_map_file, zol_results_dir, zol_high_quality_preset,
							     mibig_dir, recon_result_file, recon_og_result_file, recon_pop_color_file,
								 sociate_result_file, final_spreadsheet_xlsx, scratch_dir, logObject)

	# Close logging object and exit
	msg = 'lsaBGC-Pan completed! Check out the major results in the folder: %s' % final_dir
	logObject.info(msg)
	sys.stdout.write(msg + '\n')
	util.closeLoggerObject(logObject)
	sys.exit(0)

if __name__ == '__main__':
	lsaBGC()